{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Outex-BCNN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"wKNXglqgPhun","executionInfo":{"status":"ok","timestamp":1626686488475,"user_tz":-330,"elapsed":4384,"user":{"displayName":"NEIL PARESH MEHTA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsKd7M0H1Ghtta64n6eNaGRg03HOsuCRSk0KNj1A=s64","userId":"02534006225616056494"}}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing import image_dataset_from_directory\n","from tensorflow.keras.preprocessing import image\n","import numpy as np \n","import pandas as pd\n","from os import listdir\n","from PIL import Image as PImagea\n","from skimage.transform import resize\n","from random import shuffle\n","import cv2\n","import sklearn\n","import numpy as np\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import math\n","import glob\n","import tensorflow as tf\n","from keras.models import Sequential\n","from keras.models import Model\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, TensorBoard\n","from keras import optimizers, losses, activations, models\n","from keras.layers import Convolution2D, Dense, Input, Flatten, Dropout, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D, Concatenate\n","from keras import applications\n","from keras.applications.inception_v3 import preprocess_input\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import train_test_split\n","from matplotlib.image import NonUniformImage\n","from google.colab.patches import cv2_imshow\n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","from tensorflow.keras.applications.inception_v3 import decode_predictions\n","from tensorflow.keras.layers import Input\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J4v27S3-eugh","executionInfo":{"status":"ok","timestamp":1626686512305,"user_tz":-330,"elapsed":23858,"user":{"displayName":"NEIL PARESH MEHTA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsKd7M0H1Ghtta64n6eNaGRg03HOsuCRSk0KNj1A=s64","userId":"02534006225616056494"}},"outputId":"f6c82170-9983-4f85-9232-c8e99de06e5a"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tFBnt-Qwer-Z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625892997798,"user_tz":-330,"elapsed":1138930,"user":{"displayName":"NEIL PARESH MEHTA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsKd7M0H1Ghtta64n6eNaGRg03HOsuCRSk0KNj1A=s64","userId":"02534006225616056494"}},"outputId":"73693286-9eb0-44bb-841d-da96d63c73c0"},"source":["database = '/content/drive/MyDrive/PS I G3/Dataset/OuTex/Outex/Outex-TC-00010/000/'\n","\n","data = []\n","labels = []\n","\n","# classes\n","with open(database + \"classes.txt\", \"r\") as file:\n","    numClasses = int(file.readline())\n","    classes = {}\n","    \n","    for line in file:\n","        columns = line.split()\n","        classes[int(columns[1])] = columns[0]\n","\n","# train\n","print(\"Training steps:\")\n","with open(database + \"test_org.txt\", \"r\") as train:\n","    with open(database + \"test_files_org.txt\", \"r\") as trainFiles:\n","        numTrain = train.readline()\n","        print(numTrain)\n","        i = 0\n","        for pathFile in trainFiles:\n","            pathFile= pathFile.strip()\n","            pathFile = \"/\" + pathFile[1:]\n","            image=tf.keras.preprocessing.image.load_img(pathFile, color_mode=\"rgb\", target_size= (448,448))\n","            image=np.array(image)\n","            data.append(image)\n","            print(i)\n","            i+=1\n","            label = int(train.readline().split()[1])\n","            labels.append(label)\n","            print()\n","            \n","data = np.array(data)\n","labels = np.array(labels)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","1340\n","\n","1341\n","\n","1342\n","\n","1343\n","\n","1344\n","\n","1345\n","\n","1346\n","\n","1347\n","\n","1348\n","\n","1349\n","\n","1350\n","\n","1351\n","\n","1352\n","\n","1353\n","\n","1354\n","\n","1355\n","\n","1356\n","\n","1357\n","\n","1358\n","\n","1359\n","\n","1360\n","\n","1361\n","\n","1362\n","\n","1363\n","\n","1364\n","\n","1365\n","\n","1366\n","\n","1367\n","\n","1368\n","\n","1369\n","\n","1370\n","\n","1371\n","\n","1372\n","\n","1373\n","\n","1374\n","\n","1375\n","\n","1376\n","\n","1377\n","\n","1378\n","\n","1379\n","\n","1380\n","\n","1381\n","\n","1382\n","\n","1383\n","\n","1384\n","\n","1385\n","\n","1386\n","\n","1387\n","\n","1388\n","\n","1389\n","\n","1390\n","\n","1391\n","\n","1392\n","\n","1393\n","\n","1394\n","\n","1395\n","\n","1396\n","\n","1397\n","\n","1398\n","\n","1399\n","\n","1400\n","\n","1401\n","\n","1402\n","\n","1403\n","\n","1404\n","\n","1405\n","\n","1406\n","\n","1407\n","\n","1408\n","\n","1409\n","\n","1410\n","\n","1411\n","\n","1412\n","\n","1413\n","\n","1414\n","\n","1415\n","\n","1416\n","\n","1417\n","\n","1418\n","\n","1419\n","\n","1420\n","\n","1421\n","\n","1422\n","\n","1423\n","\n","1424\n","\n","1425\n","\n","1426\n","\n","1427\n","\n","1428\n","\n","1429\n","\n","1430\n","\n","1431\n","\n","1432\n","\n","1433\n","\n","1434\n","\n","1435\n","\n","1436\n","\n","1437\n","\n","1438\n","\n","1439\n","\n","1440\n","\n","1441\n","\n","1442\n","\n","1443\n","\n","1444\n","\n","1445\n","\n","1446\n","\n","1447\n","\n","1448\n","\n","1449\n","\n","1450\n","\n","1451\n","\n","1452\n","\n","1453\n","\n","1454\n","\n","1455\n","\n","1456\n","\n","1457\n","\n","1458\n","\n","1459\n","\n","1460\n","\n","1461\n","\n","1462\n","\n","1463\n","\n","1464\n","\n","1465\n","\n","1466\n","\n","1467\n","\n","1468\n","\n","1469\n","\n","1470\n","\n","1471\n","\n","1472\n","\n","1473\n","\n","1474\n","\n","1475\n","\n","1476\n","\n","1477\n","\n","1478\n","\n","1479\n","\n","1480\n","\n","1481\n","\n","1482\n","\n","1483\n","\n","1484\n","\n","1485\n","\n","1486\n","\n","1487\n","\n","1488\n","\n","1489\n","\n","1490\n","\n","1491\n","\n","1492\n","\n","1493\n","\n","1494\n","\n","1495\n","\n","1496\n","\n","1497\n","\n","1498\n","\n","1499\n","\n","1500\n","\n","1501\n","\n","1502\n","\n","1503\n","\n","1504\n","\n","1505\n","\n","1506\n","\n","1507\n","\n","1508\n","\n","1509\n","\n","1510\n","\n","1511\n","\n","1512\n","\n","1513\n","\n","1514\n","\n","1515\n","\n","1516\n","\n","1517\n","\n","1518\n","\n","1519\n","\n","1520\n","\n","1521\n","\n","1522\n","\n","1523\n","\n","1524\n","\n","1525\n","\n","1526\n","\n","1527\n","\n","1528\n","\n","1529\n","\n","1530\n","\n","1531\n","\n","1532\n","\n","1533\n","\n","1534\n","\n","1535\n","\n","1536\n","\n","1537\n","\n","1538\n","\n","1539\n","\n","1540\n","\n","1541\n","\n","1542\n","\n","1543\n","\n","1544\n","\n","1545\n","\n","1546\n","\n","1547\n","\n","1548\n","\n","1549\n","\n","1550\n","\n","1551\n","\n","1552\n","\n","1553\n","\n","1554\n","\n","1555\n","\n","1556\n","\n","1557\n","\n","1558\n","\n","1559\n","\n","1560\n","\n","1561\n","\n","1562\n","\n","1563\n","\n","1564\n","\n","1565\n","\n","1566\n","\n","1567\n","\n","1568\n","\n","1569\n","\n","1570\n","\n","1571\n","\n","1572\n","\n","1573\n","\n","1574\n","\n","1575\n","\n","1576\n","\n","1577\n","\n","1578\n","\n","1579\n","\n","1580\n","\n","1581\n","\n","1582\n","\n","1583\n","\n","1584\n","\n","1585\n","\n","1586\n","\n","1587\n","\n","1588\n","\n","1589\n","\n","1590\n","\n","1591\n","\n","1592\n","\n","1593\n","\n","1594\n","\n","1595\n","\n","1596\n","\n","1597\n","\n","1598\n","\n","1599\n","\n","1600\n","\n","1601\n","\n","1602\n","\n","1603\n","\n","1604\n","\n","1605\n","\n","1606\n","\n","1607\n","\n","1608\n","\n","1609\n","\n","1610\n","\n","1611\n","\n","1612\n","\n","1613\n","\n","1614\n","\n","1615\n","\n","1616\n","\n","1617\n","\n","1618\n","\n","1619\n","\n","1620\n","\n","1621\n","\n","1622\n","\n","1623\n","\n","1624\n","\n","1625\n","\n","1626\n","\n","1627\n","\n","1628\n","\n","1629\n","\n","1630\n","\n","1631\n","\n","1632\n","\n","1633\n","\n","1634\n","\n","1635\n","\n","1636\n","\n","1637\n","\n","1638\n","\n","1639\n","\n","1640\n","\n","1641\n","\n","1642\n","\n","1643\n","\n","1644\n","\n","1645\n","\n","1646\n","\n","1647\n","\n","1648\n","\n","1649\n","\n","1650\n","\n","1651\n","\n","1652\n","\n","1653\n","\n","1654\n","\n","1655\n","\n","1656\n","\n","1657\n","\n","1658\n","\n","1659\n","\n","1660\n","\n","1661\n","\n","1662\n","\n","1663\n","\n","1664\n","\n","1665\n","\n","1666\n","\n","1667\n","\n","1668\n","\n","1669\n","\n","1670\n","\n","1671\n","\n","1672\n","\n","1673\n","\n","1674\n","\n","1675\n","\n","1676\n","\n","1677\n","\n","1678\n","\n","1679\n","\n","1680\n","\n","1681\n","\n","1682\n","\n","1683\n","\n","1684\n","\n","1685\n","\n","1686\n","\n","1687\n","\n","1688\n","\n","1689\n","\n","1690\n","\n","1691\n","\n","1692\n","\n","1693\n","\n","1694\n","\n","1695\n","\n","1696\n","\n","1697\n","\n","1698\n","\n","1699\n","\n","1700\n","\n","1701\n","\n","1702\n","\n","1703\n","\n","1704\n","\n","1705\n","\n","1706\n","\n","1707\n","\n","1708\n","\n","1709\n","\n","1710\n","\n","1711\n","\n","1712\n","\n","1713\n","\n","1714\n","\n","1715\n","\n","1716\n","\n","1717\n","\n","1718\n","\n","1719\n","\n","1720\n","\n","1721\n","\n","1722\n","\n","1723\n","\n","1724\n","\n","1725\n","\n","1726\n","\n","1727\n","\n","1728\n","\n","1729\n","\n","1730\n","\n","1731\n","\n","1732\n","\n","1733\n","\n","1734\n","\n","1735\n","\n","1736\n","\n","1737\n","\n","1738\n","\n","1739\n","\n","1740\n","\n","1741\n","\n","1742\n","\n","1743\n","\n","1744\n","\n","1745\n","\n","1746\n","\n","1747\n","\n","1748\n","\n","1749\n","\n","1750\n","\n","1751\n","\n","1752\n","\n","1753\n","\n","1754\n","\n","1755\n","\n","1756\n","\n","1757\n","\n","1758\n","\n","1759\n","\n","1760\n","\n","1761\n","\n","1762\n","\n","1763\n","\n","1764\n","\n","1765\n","\n","1766\n","\n","1767\n","\n","1768\n","\n","1769\n","\n","1770\n","\n","1771\n","\n","1772\n","\n","1773\n","\n","1774\n","\n","1775\n","\n","1776\n","\n","1777\n","\n","1778\n","\n","1779\n","\n","1780\n","\n","1781\n","\n","1782\n","\n","1783\n","\n","1784\n","\n","1785\n","\n","1786\n","\n","1787\n","\n","1788\n","\n","1789\n","\n","1790\n","\n","1791\n","\n","1792\n","\n","1793\n","\n","1794\n","\n","1795\n","\n","1796\n","\n","1797\n","\n","1798\n","\n","1799\n","\n","1800\n","\n","1801\n","\n","1802\n","\n","1803\n","\n","1804\n","\n","1805\n","\n","1806\n","\n","1807\n","\n","1808\n","\n","1809\n","\n","1810\n","\n","1811\n","\n","1812\n","\n","1813\n","\n","1814\n","\n","1815\n","\n","1816\n","\n","1817\n","\n","1818\n","\n","1819\n","\n","1820\n","\n","1821\n","\n","1822\n","\n","1823\n","\n","1824\n","\n","1825\n","\n","1826\n","\n","1827\n","\n","1828\n","\n","1829\n","\n","1830\n","\n","1831\n","\n","1832\n","\n","1833\n","\n","1834\n","\n","1835\n","\n","1836\n","\n","1837\n","\n","1838\n","\n","1839\n","\n","1840\n","\n","1841\n","\n","1842\n","\n","1843\n","\n","1844\n","\n","1845\n","\n","1846\n","\n","1847\n","\n","1848\n","\n","1849\n","\n","1850\n","\n","1851\n","\n","1852\n","\n","1853\n","\n","1854\n","\n","1855\n","\n","1856\n","\n","1857\n","\n","1858\n","\n","1859\n","\n","1860\n","\n","1861\n","\n","1862\n","\n","1863\n","\n","1864\n","\n","1865\n","\n","1866\n","\n","1867\n","\n","1868\n","\n","1869\n","\n","1870\n","\n","1871\n","\n","1872\n","\n","1873\n","\n","1874\n","\n","1875\n","\n","1876\n","\n","1877\n","\n","1878\n","\n","1879\n","\n","1880\n","\n","1881\n","\n","1882\n","\n","1883\n","\n","1884\n","\n","1885\n","\n","1886\n","\n","1887\n","\n","1888\n","\n","1889\n","\n","1890\n","\n","1891\n","\n","1892\n","\n","1893\n","\n","1894\n","\n","1895\n","\n","1896\n","\n","1897\n","\n","1898\n","\n","1899\n","\n","1900\n","\n","1901\n","\n","1902\n","\n","1903\n","\n","1904\n","\n","1905\n","\n","1906\n","\n","1907\n","\n","1908\n","\n","1909\n","\n","1910\n","\n","1911\n","\n","1912\n","\n","1913\n","\n","1914\n","\n","1915\n","\n","1916\n","\n","1917\n","\n","1918\n","\n","1919\n","\n","1920\n","\n","1921\n","\n","1922\n","\n","1923\n","\n","1924\n","\n","1925\n","\n","1926\n","\n","1927\n","\n","1928\n","\n","1929\n","\n","1930\n","\n","1931\n","\n","1932\n","\n","1933\n","\n","1934\n","\n","1935\n","\n","1936\n","\n","1937\n","\n","1938\n","\n","1939\n","\n","1940\n","\n","1941\n","\n","1942\n","\n","1943\n","\n","1944\n","\n","1945\n","\n","1946\n","\n","1947\n","\n","1948\n","\n","1949\n","\n","1950\n","\n","1951\n","\n","1952\n","\n","1953\n","\n","1954\n","\n","1955\n","\n","1956\n","\n","1957\n","\n","1958\n","\n","1959\n","\n","1960\n","\n","1961\n","\n","1962\n","\n","1963\n","\n","1964\n","\n","1965\n","\n","1966\n","\n","1967\n","\n","1968\n","\n","1969\n","\n","1970\n","\n","1971\n","\n","1972\n","\n","1973\n","\n","1974\n","\n","1975\n","\n","1976\n","\n","1977\n","\n","1978\n","\n","1979\n","\n","1980\n","\n","1981\n","\n","1982\n","\n","1983\n","\n","1984\n","\n","1985\n","\n","1986\n","\n","1987\n","\n","1988\n","\n","1989\n","\n","1990\n","\n","1991\n","\n","1992\n","\n","1993\n","\n","1994\n","\n","1995\n","\n","1996\n","\n","1997\n","\n","1998\n","\n","1999\n","\n","2000\n","\n","2001\n","\n","2002\n","\n","2003\n","\n","2004\n","\n","2005\n","\n","2006\n","\n","2007\n","\n","2008\n","\n","2009\n","\n","2010\n","\n","2011\n","\n","2012\n","\n","2013\n","\n","2014\n","\n","2015\n","\n","2016\n","\n","2017\n","\n","2018\n","\n","2019\n","\n","2020\n","\n","2021\n","\n","2022\n","\n","2023\n","\n","2024\n","\n","2025\n","\n","2026\n","\n","2027\n","\n","2028\n","\n","2029\n","\n","2030\n","\n","2031\n","\n","2032\n","\n","2033\n","\n","2034\n","\n","2035\n","\n","2036\n","\n","2037\n","\n","2038\n","\n","2039\n","\n","2040\n","\n","2041\n","\n","2042\n","\n","2043\n","\n","2044\n","\n","2045\n","\n","2046\n","\n","2047\n","\n","2048\n","\n","2049\n","\n","2050\n","\n","2051\n","\n","2052\n","\n","2053\n","\n","2054\n","\n","2055\n","\n","2056\n","\n","2057\n","\n","2058\n","\n","2059\n","\n","2060\n","\n","2061\n","\n","2062\n","\n","2063\n","\n","2064\n","\n","2065\n","\n","2066\n","\n","2067\n","\n","2068\n","\n","2069\n","\n","2070\n","\n","2071\n","\n","2072\n","\n","2073\n","\n","2074\n","\n","2075\n","\n","2076\n","\n","2077\n","\n","2078\n","\n","2079\n","\n","2080\n","\n","2081\n","\n","2082\n","\n","2083\n","\n","2084\n","\n","2085\n","\n","2086\n","\n","2087\n","\n","2088\n","\n","2089\n","\n","2090\n","\n","2091\n","\n","2092\n","\n","2093\n","\n","2094\n","\n","2095\n","\n","2096\n","\n","2097\n","\n","2098\n","\n","2099\n","\n","2100\n","\n","2101\n","\n","2102\n","\n","2103\n","\n","2104\n","\n","2105\n","\n","2106\n","\n","2107\n","\n","2108\n","\n","2109\n","\n","2110\n","\n","2111\n","\n","2112\n","\n","2113\n","\n","2114\n","\n","2115\n","\n","2116\n","\n","2117\n","\n","2118\n","\n","2119\n","\n","2120\n","\n","2121\n","\n","2122\n","\n","2123\n","\n","2124\n","\n","2125\n","\n","2126\n","\n","2127\n","\n","2128\n","\n","2129\n","\n","2130\n","\n","2131\n","\n","2132\n","\n","2133\n","\n","2134\n","\n","2135\n","\n","2136\n","\n","2137\n","\n","2138\n","\n","2139\n","\n","2140\n","\n","2141\n","\n","2142\n","\n","2143\n","\n","2144\n","\n","2145\n","\n","2146\n","\n","2147\n","\n","2148\n","\n","2149\n","\n","2150\n","\n","2151\n","\n","2152\n","\n","2153\n","\n","2154\n","\n","2155\n","\n","2156\n","\n","2157\n","\n","2158\n","\n","2159\n","\n","2160\n","\n","2161\n","\n","2162\n","\n","2163\n","\n","2164\n","\n","2165\n","\n","2166\n","\n","2167\n","\n","2168\n","\n","2169\n","\n","2170\n","\n","2171\n","\n","2172\n","\n","2173\n","\n","2174\n","\n","2175\n","\n","2176\n","\n","2177\n","\n","2178\n","\n","2179\n","\n","2180\n","\n","2181\n","\n","2182\n","\n","2183\n","\n","2184\n","\n","2185\n","\n","2186\n","\n","2187\n","\n","2188\n","\n","2189\n","\n","2190\n","\n","2191\n","\n","2192\n","\n","2193\n","\n","2194\n","\n","2195\n","\n","2196\n","\n","2197\n","\n","2198\n","\n","2199\n","\n","2200\n","\n","2201\n","\n","2202\n","\n","2203\n","\n","2204\n","\n","2205\n","\n","2206\n","\n","2207\n","\n","2208\n","\n","2209\n","\n","2210\n","\n","2211\n","\n","2212\n","\n","2213\n","\n","2214\n","\n","2215\n","\n","2216\n","\n","2217\n","\n","2218\n","\n","2219\n","\n","2220\n","\n","2221\n","\n","2222\n","\n","2223\n","\n","2224\n","\n","2225\n","\n","2226\n","\n","2227\n","\n","2228\n","\n","2229\n","\n","2230\n","\n","2231\n","\n","2232\n","\n","2233\n","\n","2234\n","\n","2235\n","\n","2236\n","\n","2237\n","\n","2238\n","\n","2239\n","\n","2240\n","\n","2241\n","\n","2242\n","\n","2243\n","\n","2244\n","\n","2245\n","\n","2246\n","\n","2247\n","\n","2248\n","\n","2249\n","\n","2250\n","\n","2251\n","\n","2252\n","\n","2253\n","\n","2254\n","\n","2255\n","\n","2256\n","\n","2257\n","\n","2258\n","\n","2259\n","\n","2260\n","\n","2261\n","\n","2262\n","\n","2263\n","\n","2264\n","\n","2265\n","\n","2266\n","\n","2267\n","\n","2268\n","\n","2269\n","\n","2270\n","\n","2271\n","\n","2272\n","\n","2273\n","\n","2274\n","\n","2275\n","\n","2276\n","\n","2277\n","\n","2278\n","\n","2279\n","\n","2280\n","\n","2281\n","\n","2282\n","\n","2283\n","\n","2284\n","\n","2285\n","\n","2286\n","\n","2287\n","\n","2288\n","\n","2289\n","\n","2290\n","\n","2291\n","\n","2292\n","\n","2293\n","\n","2294\n","\n","2295\n","\n","2296\n","\n","2297\n","\n","2298\n","\n","2299\n","\n","2300\n","\n","2301\n","\n","2302\n","\n","2303\n","\n","2304\n","\n","2305\n","\n","2306\n","\n","2307\n","\n","2308\n","\n","2309\n","\n","2310\n","\n","2311\n","\n","2312\n","\n","2313\n","\n","2314\n","\n","2315\n","\n","2316\n","\n","2317\n","\n","2318\n","\n","2319\n","\n","2320\n","\n","2321\n","\n","2322\n","\n","2323\n","\n","2324\n","\n","2325\n","\n","2326\n","\n","2327\n","\n","2328\n","\n","2329\n","\n","2330\n","\n","2331\n","\n","2332\n","\n","2333\n","\n","2334\n","\n","2335\n","\n","2336\n","\n","2337\n","\n","2338\n","\n","2339\n","\n","2340\n","\n","2341\n","\n","2342\n","\n","2343\n","\n","2344\n","\n","2345\n","\n","2346\n","\n","2347\n","\n","2348\n","\n","2349\n","\n","2350\n","\n","2351\n","\n","2352\n","\n","2353\n","\n","2354\n","\n","2355\n","\n","2356\n","\n","2357\n","\n","2358\n","\n","2359\n","\n","2360\n","\n","2361\n","\n","2362\n","\n","2363\n","\n","2364\n","\n","2365\n","\n","2366\n","\n","2367\n","\n","2368\n","\n","2369\n","\n","2370\n","\n","2371\n","\n","2372\n","\n","2373\n","\n","2374\n","\n","2375\n","\n","2376\n","\n","2377\n","\n","2378\n","\n","2379\n","\n","2380\n","\n","2381\n","\n","2382\n","\n","2383\n","\n","2384\n","\n","2385\n","\n","2386\n","\n","2387\n","\n","2388\n","\n","2389\n","\n","2390\n","\n","2391\n","\n","2392\n","\n","2393\n","\n","2394\n","\n","2395\n","\n","2396\n","\n","2397\n","\n","2398\n","\n","2399\n","\n","2400\n","\n","2401\n","\n","2402\n","\n","2403\n","\n","2404\n","\n","2405\n","\n","2406\n","\n","2407\n","\n","2408\n","\n","2409\n","\n","2410\n","\n","2411\n","\n","2412\n","\n","2413\n","\n","2414\n","\n","2415\n","\n","2416\n","\n","2417\n","\n","2418\n","\n","2419\n","\n","2420\n","\n","2421\n","\n","2422\n","\n","2423\n","\n","2424\n","\n","2425\n","\n","2426\n","\n","2427\n","\n","2428\n","\n","2429\n","\n","2430\n","\n","2431\n","\n","2432\n","\n","2433\n","\n","2434\n","\n","2435\n","\n","2436\n","\n","2437\n","\n","2438\n","\n","2439\n","\n","2440\n","\n","2441\n","\n","2442\n","\n","2443\n","\n","2444\n","\n","2445\n","\n","2446\n","\n","2447\n","\n","2448\n","\n","2449\n","\n","2450\n","\n","2451\n","\n","2452\n","\n","2453\n","\n","2454\n","\n","2455\n","\n","2456\n","\n","2457\n","\n","2458\n","\n","2459\n","\n","2460\n","\n","2461\n","\n","2462\n","\n","2463\n","\n","2464\n","\n","2465\n","\n","2466\n","\n","2467\n","\n","2468\n","\n","2469\n","\n","2470\n","\n","2471\n","\n","2472\n","\n","2473\n","\n","2474\n","\n","2475\n","\n","2476\n","\n","2477\n","\n","2478\n","\n","2479\n","\n","2480\n","\n","2481\n","\n","2482\n","\n","2483\n","\n","2484\n","\n","2485\n","\n","2486\n","\n","2487\n","\n","2488\n","\n","2489\n","\n","2490\n","\n","2491\n","\n","2492\n","\n","2493\n","\n","2494\n","\n","2495\n","\n","2496\n","\n","2497\n","\n","2498\n","\n","2499\n","\n","2500\n","\n","2501\n","\n","2502\n","\n","2503\n","\n","2504\n","\n","2505\n","\n","2506\n","\n","2507\n","\n","2508\n","\n","2509\n","\n","2510\n","\n","2511\n","\n","2512\n","\n","2513\n","\n","2514\n","\n","2515\n","\n","2516\n","\n","2517\n","\n","2518\n","\n","2519\n","\n","2520\n","\n","2521\n","\n","2522\n","\n","2523\n","\n","2524\n","\n","2525\n","\n","2526\n","\n","2527\n","\n","2528\n","\n","2529\n","\n","2530\n","\n","2531\n","\n","2532\n","\n","2533\n","\n","2534\n","\n","2535\n","\n","2536\n","\n","2537\n","\n","2538\n","\n","2539\n","\n","2540\n","\n","2541\n","\n","2542\n","\n","2543\n","\n","2544\n","\n","2545\n","\n","2546\n","\n","2547\n","\n","2548\n","\n","2549\n","\n","2550\n","\n","2551\n","\n","2552\n","\n","2553\n","\n","2554\n","\n","2555\n","\n","2556\n","\n","2557\n","\n","2558\n","\n","2559\n","\n","2560\n","\n","2561\n","\n","2562\n","\n","2563\n","\n","2564\n","\n","2565\n","\n","2566\n","\n","2567\n","\n","2568\n","\n","2569\n","\n","2570\n","\n","2571\n","\n","2572\n","\n","2573\n","\n","2574\n","\n","2575\n","\n","2576\n","\n","2577\n","\n","2578\n","\n","2579\n","\n","2580\n","\n","2581\n","\n","2582\n","\n","2583\n","\n","2584\n","\n","2585\n","\n","2586\n","\n","2587\n","\n","2588\n","\n","2589\n","\n","2590\n","\n","2591\n","\n","2592\n","\n","2593\n","\n","2594\n","\n","2595\n","\n","2596\n","\n","2597\n","\n","2598\n","\n","2599\n","\n","2600\n","\n","2601\n","\n","2602\n","\n","2603\n","\n","2604\n","\n","2605\n","\n","2606\n","\n","2607\n","\n","2608\n","\n","2609\n","\n","2610\n","\n","2611\n","\n","2612\n","\n","2613\n","\n","2614\n","\n","2615\n","\n","2616\n","\n","2617\n","\n","2618\n","\n","2619\n","\n","2620\n","\n","2621\n","\n","2622\n","\n","2623\n","\n","2624\n","\n","2625\n","\n","2626\n","\n","2627\n","\n","2628\n","\n","2629\n","\n","2630\n","\n","2631\n","\n","2632\n","\n","2633\n","\n","2634\n","\n","2635\n","\n","2636\n","\n","2637\n","\n","2638\n","\n","2639\n","\n","2640\n","\n","2641\n","\n","2642\n","\n","2643\n","\n","2644\n","\n","2645\n","\n","2646\n","\n","2647\n","\n","2648\n","\n","2649\n","\n","2650\n","\n","2651\n","\n","2652\n","\n","2653\n","\n","2654\n","\n","2655\n","\n","2656\n","\n","2657\n","\n","2658\n","\n","2659\n","\n","2660\n","\n","2661\n","\n","2662\n","\n","2663\n","\n","2664\n","\n","2665\n","\n","2666\n","\n","2667\n","\n","2668\n","\n","2669\n","\n","2670\n","\n","2671\n","\n","2672\n","\n","2673\n","\n","2674\n","\n","2675\n","\n","2676\n","\n","2677\n","\n","2678\n","\n","2679\n","\n","2680\n","\n","2681\n","\n","2682\n","\n","2683\n","\n","2684\n","\n","2685\n","\n","2686\n","\n","2687\n","\n","2688\n","\n","2689\n","\n","2690\n","\n","2691\n","\n","2692\n","\n","2693\n","\n","2694\n","\n","2695\n","\n","2696\n","\n","2697\n","\n","2698\n","\n","2699\n","\n","2700\n","\n","2701\n","\n","2702\n","\n","2703\n","\n","2704\n","\n","2705\n","\n","2706\n","\n","2707\n","\n","2708\n","\n","2709\n","\n","2710\n","\n","2711\n","\n","2712\n","\n","2713\n","\n","2714\n","\n","2715\n","\n","2716\n","\n","2717\n","\n","2718\n","\n","2719\n","\n","2720\n","\n","2721\n","\n","2722\n","\n","2723\n","\n","2724\n","\n","2725\n","\n","2726\n","\n","2727\n","\n","2728\n","\n","2729\n","\n","2730\n","\n","2731\n","\n","2732\n","\n","2733\n","\n","2734\n","\n","2735\n","\n","2736\n","\n","2737\n","\n","2738\n","\n","2739\n","\n","2740\n","\n","2741\n","\n","2742\n","\n","2743\n","\n","2744\n","\n","2745\n","\n","2746\n","\n","2747\n","\n","2748\n","\n","2749\n","\n","2750\n","\n","2751\n","\n","2752\n","\n","2753\n","\n","2754\n","\n","2755\n","\n","2756\n","\n","2757\n","\n","2758\n","\n","2759\n","\n","2760\n","\n","2761\n","\n","2762\n","\n","2763\n","\n","2764\n","\n","2765\n","\n","2766\n","\n","2767\n","\n","2768\n","\n","2769\n","\n","2770\n","\n","2771\n","\n","2772\n","\n","2773\n","\n","2774\n","\n","2775\n","\n","2776\n","\n","2777\n","\n","2778\n","\n","2779\n","\n","2780\n","\n","2781\n","\n","2782\n","\n","2783\n","\n","2784\n","\n","2785\n","\n","2786\n","\n","2787\n","\n","2788\n","\n","2789\n","\n","2790\n","\n","2791\n","\n","2792\n","\n","2793\n","\n","2794\n","\n","2795\n","\n","2796\n","\n","2797\n","\n","2798\n","\n","2799\n","\n","2800\n","\n","2801\n","\n","2802\n","\n","2803\n","\n","2804\n","\n","2805\n","\n","2806\n","\n","2807\n","\n","2808\n","\n","2809\n","\n","2810\n","\n","2811\n","\n","2812\n","\n","2813\n","\n","2814\n","\n","2815\n","\n","2816\n","\n","2817\n","\n","2818\n","\n","2819\n","\n","2820\n","\n","2821\n","\n","2822\n","\n","2823\n","\n","2824\n","\n","2825\n","\n","2826\n","\n","2827\n","\n","2828\n","\n","2829\n","\n","2830\n","\n","2831\n","\n","2832\n","\n","2833\n","\n","2834\n","\n","2835\n","\n","2836\n","\n","2837\n","\n","2838\n","\n","2839\n","\n","2840\n","\n","2841\n","\n","2842\n","\n","2843\n","\n","2844\n","\n","2845\n","\n","2846\n","\n","2847\n","\n","2848\n","\n","2849\n","\n","2850\n","\n","2851\n","\n","2852\n","\n","2853\n","\n","2854\n","\n","2855\n","\n","2856\n","\n","2857\n","\n","2858\n","\n","2859\n","\n","2860\n","\n","2861\n","\n","2862\n","\n","2863\n","\n","2864\n","\n","2865\n","\n","2866\n","\n","2867\n","\n","2868\n","\n","2869\n","\n","2870\n","\n","2871\n","\n","2872\n","\n","2873\n","\n","2874\n","\n","2875\n","\n","2876\n","\n","2877\n","\n","2878\n","\n","2879\n","\n","2880\n","\n","2881\n","\n","2882\n","\n","2883\n","\n","2884\n","\n","2885\n","\n","2886\n","\n","2887\n","\n","2888\n","\n","2889\n","\n","2890\n","\n","2891\n","\n","2892\n","\n","2893\n","\n","2894\n","\n","2895\n","\n","2896\n","\n","2897\n","\n","2898\n","\n","2899\n","\n","2900\n","\n","2901\n","\n","2902\n","\n","2903\n","\n","2904\n","\n","2905\n","\n","2906\n","\n","2907\n","\n","2908\n","\n","2909\n","\n","2910\n","\n","2911\n","\n","2912\n","\n","2913\n","\n","2914\n","\n","2915\n","\n","2916\n","\n","2917\n","\n","2918\n","\n","2919\n","\n","2920\n","\n","2921\n","\n","2922\n","\n","2923\n","\n","2924\n","\n","2925\n","\n","2926\n","\n","2927\n","\n","2928\n","\n","2929\n","\n","2930\n","\n","2931\n","\n","2932\n","\n","2933\n","\n","2934\n","\n","2935\n","\n","2936\n","\n","2937\n","\n","2938\n","\n","2939\n","\n","2940\n","\n","2941\n","\n","2942\n","\n","2943\n","\n","2944\n","\n","2945\n","\n","2946\n","\n","2947\n","\n","2948\n","\n","2949\n","\n","2950\n","\n","2951\n","\n","2952\n","\n","2953\n","\n","2954\n","\n","2955\n","\n","2956\n","\n","2957\n","\n","2958\n","\n","2959\n","\n","2960\n","\n","2961\n","\n","2962\n","\n","2963\n","\n","2964\n","\n","2965\n","\n","2966\n","\n","2967\n","\n","2968\n","\n","2969\n","\n","2970\n","\n","2971\n","\n","2972\n","\n","2973\n","\n","2974\n","\n","2975\n","\n","2976\n","\n","2977\n","\n","2978\n","\n","2979\n","\n","2980\n","\n","2981\n","\n","2982\n","\n","2983\n","\n","2984\n","\n","2985\n","\n","2986\n","\n","2987\n","\n","2988\n","\n","2989\n","\n","2990\n","\n","2991\n","\n","2992\n","\n","2993\n","\n","2994\n","\n","2995\n","\n","2996\n","\n","2997\n","\n","2998\n","\n","2999\n","\n","3000\n","\n","3001\n","\n","3002\n","\n","3003\n","\n","3004\n","\n","3005\n","\n","3006\n","\n","3007\n","\n","3008\n","\n","3009\n","\n","3010\n","\n","3011\n","\n","3012\n","\n","3013\n","\n","3014\n","\n","3015\n","\n","3016\n","\n","3017\n","\n","3018\n","\n","3019\n","\n","3020\n","\n","3021\n","\n","3022\n","\n","3023\n","\n","3024\n","\n","3025\n","\n","3026\n","\n","3027\n","\n","3028\n","\n","3029\n","\n","3030\n","\n","3031\n","\n","3032\n","\n","3033\n","\n","3034\n","\n","3035\n","\n","3036\n","\n","3037\n","\n","3038\n","\n","3039\n","\n","3040\n","\n","3041\n","\n","3042\n","\n","3043\n","\n","3044\n","\n","3045\n","\n","3046\n","\n","3047\n","\n","3048\n","\n","3049\n","\n","3050\n","\n","3051\n","\n","3052\n","\n","3053\n","\n","3054\n","\n","3055\n","\n","3056\n","\n","3057\n","\n","3058\n","\n","3059\n","\n","3060\n","\n","3061\n","\n","3062\n","\n","3063\n","\n","3064\n","\n","3065\n","\n","3066\n","\n","3067\n","\n","3068\n","\n","3069\n","\n","3070\n","\n","3071\n","\n","3072\n","\n","3073\n","\n","3074\n","\n","3075\n","\n","3076\n","\n","3077\n","\n","3078\n","\n","3079\n","\n","3080\n","\n","3081\n","\n","3082\n","\n","3083\n","\n","3084\n","\n","3085\n","\n","3086\n","\n","3087\n","\n","3088\n","\n","3089\n","\n","3090\n","\n","3091\n","\n","3092\n","\n","3093\n","\n","3094\n","\n","3095\n","\n","3096\n","\n","3097\n","\n","3098\n","\n","3099\n","\n","3100\n","\n","3101\n","\n","3102\n","\n","3103\n","\n","3104\n","\n","3105\n","\n","3106\n","\n","3107\n","\n","3108\n","\n","3109\n","\n","3110\n","\n","3111\n","\n","3112\n","\n","3113\n","\n","3114\n","\n","3115\n","\n","3116\n","\n","3117\n","\n","3118\n","\n","3119\n","\n","3120\n","\n","3121\n","\n","3122\n","\n","3123\n","\n","3124\n","\n","3125\n","\n","3126\n","\n","3127\n","\n","3128\n","\n","3129\n","\n","3130\n","\n","3131\n","\n","3132\n","\n","3133\n","\n","3134\n","\n","3135\n","\n","3136\n","\n","3137\n","\n","3138\n","\n","3139\n","\n","3140\n","\n","3141\n","\n","3142\n","\n","3143\n","\n","3144\n","\n","3145\n","\n","3146\n","\n","3147\n","\n","3148\n","\n","3149\n","\n","3150\n","\n","3151\n","\n","3152\n","\n","3153\n","\n","3154\n","\n","3155\n","\n","3156\n","\n","3157\n","\n","3158\n","\n","3159\n","\n","3160\n","\n","3161\n","\n","3162\n","\n","3163\n","\n","3164\n","\n","3165\n","\n","3166\n","\n","3167\n","\n","3168\n","\n","3169\n","\n","3170\n","\n","3171\n","\n","3172\n","\n","3173\n","\n","3174\n","\n","3175\n","\n","3176\n","\n","3177\n","\n","3178\n","\n","3179\n","\n","3180\n","\n","3181\n","\n","3182\n","\n","3183\n","\n","3184\n","\n","3185\n","\n","3186\n","\n","3187\n","\n","3188\n","\n","3189\n","\n","3190\n","\n","3191\n","\n","3192\n","\n","3193\n","\n","3194\n","\n","3195\n","\n","3196\n","\n","3197\n","\n","3198\n","\n","3199\n","\n","3200\n","\n","3201\n","\n","3202\n","\n","3203\n","\n","3204\n","\n","3205\n","\n","3206\n","\n","3207\n","\n","3208\n","\n","3209\n","\n","3210\n","\n","3211\n","\n","3212\n","\n","3213\n","\n","3214\n","\n","3215\n","\n","3216\n","\n","3217\n","\n","3218\n","\n","3219\n","\n","3220\n","\n","3221\n","\n","3222\n","\n","3223\n","\n","3224\n","\n","3225\n","\n","3226\n","\n","3227\n","\n","3228\n","\n","3229\n","\n","3230\n","\n","3231\n","\n","3232\n","\n","3233\n","\n","3234\n","\n","3235\n","\n","3236\n","\n","3237\n","\n","3238\n","\n","3239\n","\n","3240\n","\n","3241\n","\n","3242\n","\n","3243\n","\n","3244\n","\n","3245\n","\n","3246\n","\n","3247\n","\n","3248\n","\n","3249\n","\n","3250\n","\n","3251\n","\n","3252\n","\n","3253\n","\n","3254\n","\n","3255\n","\n","3256\n","\n","3257\n","\n","3258\n","\n","3259\n","\n","3260\n","\n","3261\n","\n","3262\n","\n","3263\n","\n","3264\n","\n","3265\n","\n","3266\n","\n","3267\n","\n","3268\n","\n","3269\n","\n","3270\n","\n","3271\n","\n","3272\n","\n","3273\n","\n","3274\n","\n","3275\n","\n","3276\n","\n","3277\n","\n","3278\n","\n","3279\n","\n","3280\n","\n","3281\n","\n","3282\n","\n","3283\n","\n","3284\n","\n","3285\n","\n","3286\n","\n","3287\n","\n","3288\n","\n","3289\n","\n","3290\n","\n","3291\n","\n","3292\n","\n","3293\n","\n","3294\n","\n","3295\n","\n","3296\n","\n","3297\n","\n","3298\n","\n","3299\n","\n","3300\n","\n","3301\n","\n","3302\n","\n","3303\n","\n","3304\n","\n","3305\n","\n","3306\n","\n","3307\n","\n","3308\n","\n","3309\n","\n","3310\n","\n","3311\n","\n","3312\n","\n","3313\n","\n","3314\n","\n","3315\n","\n","3316\n","\n","3317\n","\n","3318\n","\n","3319\n","\n","3320\n","\n","3321\n","\n","3322\n","\n","3323\n","\n","3324\n","\n","3325\n","\n","3326\n","\n","3327\n","\n","3328\n","\n","3329\n","\n","3330\n","\n","3331\n","\n","3332\n","\n","3333\n","\n","3334\n","\n","3335\n","\n","3336\n","\n","3337\n","\n","3338\n","\n","3339\n","\n","3340\n","\n","3341\n","\n","3342\n","\n","3343\n","\n","3344\n","\n","3345\n","\n","3346\n","\n","3347\n","\n","3348\n","\n","3349\n","\n","3350\n","\n","3351\n","\n","3352\n","\n","3353\n","\n","3354\n","\n","3355\n","\n","3356\n","\n","3357\n","\n","3358\n","\n","3359\n","\n","3360\n","\n","3361\n","\n","3362\n","\n","3363\n","\n","3364\n","\n","3365\n","\n","3366\n","\n","3367\n","\n","3368\n","\n","3369\n","\n","3370\n","\n","3371\n","\n","3372\n","\n","3373\n","\n","3374\n","\n","3375\n","\n","3376\n","\n","3377\n","\n","3378\n","\n","3379\n","\n","3380\n","\n","3381\n","\n","3382\n","\n","3383\n","\n","3384\n","\n","3385\n","\n","3386\n","\n","3387\n","\n","3388\n","\n","3389\n","\n","3390\n","\n","3391\n","\n","3392\n","\n","3393\n","\n","3394\n","\n","3395\n","\n","3396\n","\n","3397\n","\n","3398\n","\n","3399\n","\n","3400\n","\n","3401\n","\n","3402\n","\n","3403\n","\n","3404\n","\n","3405\n","\n","3406\n","\n","3407\n","\n","3408\n","\n","3409\n","\n","3410\n","\n","3411\n","\n","3412\n","\n","3413\n","\n","3414\n","\n","3415\n","\n","3416\n","\n","3417\n","\n","3418\n","\n","3419\n","\n","3420\n","\n","3421\n","\n","3422\n","\n","3423\n","\n","3424\n","\n","3425\n","\n","3426\n","\n","3427\n","\n","3428\n","\n","3429\n","\n","3430\n","\n","3431\n","\n","3432\n","\n","3433\n","\n","3434\n","\n","3435\n","\n","3436\n","\n","3437\n","\n","3438\n","\n","3439\n","\n","3440\n","\n","3441\n","\n","3442\n","\n","3443\n","\n","3444\n","\n","3445\n","\n","3446\n","\n","3447\n","\n","3448\n","\n","3449\n","\n","3450\n","\n","3451\n","\n","3452\n","\n","3453\n","\n","3454\n","\n","3455\n","\n","3456\n","\n","3457\n","\n","3458\n","\n","3459\n","\n","3460\n","\n","3461\n","\n","3462\n","\n","3463\n","\n","3464\n","\n","3465\n","\n","3466\n","\n","3467\n","\n","3468\n","\n","3469\n","\n","3470\n","\n","3471\n","\n","3472\n","\n","3473\n","\n","3474\n","\n","3475\n","\n","3476\n","\n","3477\n","\n","3478\n","\n","3479\n","\n","3480\n","\n","3481\n","\n","3482\n","\n","3483\n","\n","3484\n","\n","3485\n","\n","3486\n","\n","3487\n","\n","3488\n","\n","3489\n","\n","3490\n","\n","3491\n","\n","3492\n","\n","3493\n","\n","3494\n","\n","3495\n","\n","3496\n","\n","3497\n","\n","3498\n","\n","3499\n","\n","3500\n","\n","3501\n","\n","3502\n","\n","3503\n","\n","3504\n","\n","3505\n","\n","3506\n","\n","3507\n","\n","3508\n","\n","3509\n","\n","3510\n","\n","3511\n","\n","3512\n","\n","3513\n","\n","3514\n","\n","3515\n","\n","3516\n","\n","3517\n","\n","3518\n","\n","3519\n","\n","3520\n","\n","3521\n","\n","3522\n","\n","3523\n","\n","3524\n","\n","3525\n","\n","3526\n","\n","3527\n","\n","3528\n","\n","3529\n","\n","3530\n","\n","3531\n","\n","3532\n","\n","3533\n","\n","3534\n","\n","3535\n","\n","3536\n","\n","3537\n","\n","3538\n","\n","3539\n","\n","3540\n","\n","3541\n","\n","3542\n","\n","3543\n","\n","3544\n","\n","3545\n","\n","3546\n","\n","3547\n","\n","3548\n","\n","3549\n","\n","3550\n","\n","3551\n","\n","3552\n","\n","3553\n","\n","3554\n","\n","3555\n","\n","3556\n","\n","3557\n","\n","3558\n","\n","3559\n","\n","3560\n","\n","3561\n","\n","3562\n","\n","3563\n","\n","3564\n","\n","3565\n","\n","3566\n","\n","3567\n","\n","3568\n","\n","3569\n","\n","3570\n","\n","3571\n","\n","3572\n","\n","3573\n","\n","3574\n","\n","3575\n","\n","3576\n","\n","3577\n","\n","3578\n","\n","3579\n","\n","3580\n","\n","3581\n","\n","3582\n","\n","3583\n","\n","3584\n","\n","3585\n","\n","3586\n","\n","3587\n","\n","3588\n","\n","3589\n","\n","3590\n","\n","3591\n","\n","3592\n","\n","3593\n","\n","3594\n","\n","3595\n","\n","3596\n","\n","3597\n","\n","3598\n","\n","3599\n","\n","3600\n","\n","3601\n","\n","3602\n","\n","3603\n","\n","3604\n","\n","3605\n","\n","3606\n","\n","3607\n","\n","3608\n","\n","3609\n","\n","3610\n","\n","3611\n","\n","3612\n","\n","3613\n","\n","3614\n","\n","3615\n","\n","3616\n","\n","3617\n","\n","3618\n","\n","3619\n","\n","3620\n","\n","3621\n","\n","3622\n","\n","3623\n","\n","3624\n","\n","3625\n","\n","3626\n","\n","3627\n","\n","3628\n","\n","3629\n","\n","3630\n","\n","3631\n","\n","3632\n","\n","3633\n","\n","3634\n","\n","3635\n","\n","3636\n","\n","3637\n","\n","3638\n","\n","3639\n","\n","3640\n","\n","3641\n","\n","3642\n","\n","3643\n","\n","3644\n","\n","3645\n","\n","3646\n","\n","3647\n","\n","3648\n","\n","3649\n","\n","3650\n","\n","3651\n","\n","3652\n","\n","3653\n","\n","3654\n","\n","3655\n","\n","3656\n","\n","3657\n","\n","3658\n","\n","3659\n","\n","3660\n","\n","3661\n","\n","3662\n","\n","3663\n","\n","3664\n","\n","3665\n","\n","3666\n","\n","3667\n","\n","3668\n","\n","3669\n","\n","3670\n","\n","3671\n","\n","3672\n","\n","3673\n","\n","3674\n","\n","3675\n","\n","3676\n","\n","3677\n","\n","3678\n","\n","3679\n","\n","3680\n","\n","3681\n","\n","3682\n","\n","3683\n","\n","3684\n","\n","3685\n","\n","3686\n","\n","3687\n","\n","3688\n","\n","3689\n","\n","3690\n","\n","3691\n","\n","3692\n","\n","3693\n","\n","3694\n","\n","3695\n","\n","3696\n","\n","3697\n","\n","3698\n","\n","3699\n","\n","3700\n","\n","3701\n","\n","3702\n","\n","3703\n","\n","3704\n","\n","3705\n","\n","3706\n","\n","3707\n","\n","3708\n","\n","3709\n","\n","3710\n","\n","3711\n","\n","3712\n","\n","3713\n","\n","3714\n","\n","3715\n","\n","3716\n","\n","3717\n","\n","3718\n","\n","3719\n","\n","3720\n","\n","3721\n","\n","3722\n","\n","3723\n","\n","3724\n","\n","3725\n","\n","3726\n","\n","3727\n","\n","3728\n","\n","3729\n","\n","3730\n","\n","3731\n","\n","3732\n","\n","3733\n","\n","3734\n","\n","3735\n","\n","3736\n","\n","3737\n","\n","3738\n","\n","3739\n","\n","3740\n","\n","3741\n","\n","3742\n","\n","3743\n","\n","3744\n","\n","3745\n","\n","3746\n","\n","3747\n","\n","3748\n","\n","3749\n","\n","3750\n","\n","3751\n","\n","3752\n","\n","3753\n","\n","3754\n","\n","3755\n","\n","3756\n","\n","3757\n","\n","3758\n","\n","3759\n","\n","3760\n","\n","3761\n","\n","3762\n","\n","3763\n","\n","3764\n","\n","3765\n","\n","3766\n","\n","3767\n","\n","3768\n","\n","3769\n","\n","3770\n","\n","3771\n","\n","3772\n","\n","3773\n","\n","3774\n","\n","3775\n","\n","3776\n","\n","3777\n","\n","3778\n","\n","3779\n","\n","3780\n","\n","3781\n","\n","3782\n","\n","3783\n","\n","3784\n","\n","3785\n","\n","3786\n","\n","3787\n","\n","3788\n","\n","3789\n","\n","3790\n","\n","3791\n","\n","3792\n","\n","3793\n","\n","3794\n","\n","3795\n","\n","3796\n","\n","3797\n","\n","3798\n","\n","3799\n","\n","3800\n","\n","3801\n","\n","3802\n","\n","3803\n","\n","3804\n","\n","3805\n","\n","3806\n","\n","3807\n","\n","3808\n","\n","3809\n","\n","3810\n","\n","3811\n","\n","3812\n","\n","3813\n","\n","3814\n","\n","3815\n","\n","3816\n","\n","3817\n","\n","3818\n","\n","3819\n","\n","3820\n","\n","3821\n","\n","3822\n","\n","3823\n","\n","3824\n","\n","3825\n","\n","3826\n","\n","3827\n","\n","3828\n","\n","3829\n","\n","3830\n","\n","3831\n","\n","3832\n","\n","3833\n","\n","3834\n","\n","3835\n","\n","3836\n","\n","3837\n","\n","3838\n","\n","3839\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hqeySSObWUq7"},"source":["X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p0rwNdfwo0tz","executionInfo":{"status":"ok","timestamp":1626686585958,"user_tz":-330,"elapsed":501,"user":{"displayName":"NEIL PARESH MEHTA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsKd7M0H1Ghtta64n6eNaGRg03HOsuCRSk0KNj1A=s64","userId":"02534006225616056494"}}},"source":["# rows,cols,channels = np.shape(data[0])\n","# print('rows,cols,channels = ',rows,cols,channels)\n","rows,cols,channels = (448,448,3)\n","numClasses = 24"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nkWEv-P3oI_3","executionInfo":{"status":"ok","timestamp":1625892998686,"user_tz":-330,"elapsed":27,"user":{"displayName":"NEIL PARESH MEHTA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsKd7M0H1Ghtta64n6eNaGRg03HOsuCRSk0KNj1A=s64","userId":"02534006225616056494"}},"outputId":"77988be3-de0d-48a8-8662-905ed6300ba7"},"source":["X_train.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3072, 448, 448, 3)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"sk8ov9I0-TxR"},"source":["# x_5,x_5_3 (m+d)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"MFST41cIkiIr","executionInfo":{"status":"error","timestamp":1626687589320,"user_tz":-330,"elapsed":1248,"user":{"displayName":"NEIL PARESH MEHTA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsKd7M0H1Ghtta64n6eNaGRg03HOsuCRSk0KNj1A=s64","userId":"02534006225616056494"}},"outputId":"2a2bee4a-7116-4714-ed2f-cb7fdd5dc5f8"},"source":["# model_pretrained = tf.keras.applications.InceptionV3(weights=\"imagenet\",include_top=False,input_shape=(rows,cols,3))\n","# model_pretrained.summary()\n","\n","# inp = model_pretrained.input\n","# out = model_pretrained.layers[-2].output\n","\n","from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","import numpy as np\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","import tensorflow as tf\n","\n","# create the base pre-trained model\n","model_pretrained = VGG16(weights='imagenet', include_top=False,input_shape=(rows,cols,3))\n","model_pretrained.summary()\n","\n","# first: train only the top layers (which were randomly initialized)\n","# i.e. freeze all convolutional InceptionV3 layers except the last dense \n","# layer for 11 classes that was just added.\n","for layer in model_pretrained.layers:\n","    layer.trainable = False\n","\n","x_5 = model_pretrained.get_layer(index=-2).output\n","x_5_3 = model_pretrained.get_layer(index=-5).output\n","added_layer = tf.einsum('ijkm,ijkn->imn',x_5,x_5_3)\n","x = tf.reshape(added_layer,[-1,512*512])\n","print('Shape of phi_I after reshape', x.get_shape())\n","\n","x = tf.divide(x,(784.0))  \n","print('Shape of phi_I after division', x.get_shape())\n","\n","a = tf.multiply(tf.sign(x),tf.sqrt(tf.abs(x)+1e-12))\n","print('Shape of y_ssqrt', a.get_shape())\n","\n","z = tf.nn.l2_normalize(a, dim=1)\n","print('Shape of z_l2', z.get_shape())\n","\n","\n","# x = GlobalAveragePooling2D()(x)\n","# let's add a fully-connected layer\n","# x = Dense(1024, activation='relu')(x)\n","# x = Dense(1024, activation='relu')(x)\n","\n","# and a logistic layer -- let's say we have 24 classes\n","predictions = Dense(numClasses, activation='softmax')(z)\n","\n","\n","\n","# this is the model we will train\n","bcnn_model = Model(inputs=model_pretrained.input, outputs=predictions)\n","\n","# compile the model (should be done *after* setting layers to non-trainable)\n","bcnn_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n","\n","one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=numClasses)\n","bcnn_model.summary()\n","bcnn_model.fit(x=data,y=one_hot_labels,epochs=10,batch_size=32)\n","# from keras.models import Sequential\n","# from keras.layers import Dense\n","# from keras.utils.vis_utils import plot_model\n","# # model = Sequential()\n","# # model.add(Dense(2, input_dim=1, activation='relu'))\n","# # model.add(Dense(1, activation='sigmoid'))\n","# plot_model(bcnn_model, show_shapes=True, show_layer_names=True)\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Model: \"vgg16\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_14 (InputLayer)        [(None, 448, 448, 3)]     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 448, 448, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 448, 448, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 224, 224, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 224, 224, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 224, 224, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 112, 112, 128)     0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 112, 112, 256)     295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 112, 112, 256)     590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 112, 112, 256)     590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 56, 56, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 56, 56, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 56, 56, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 56, 56, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 28, 28, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 14,714,688\n","Non-trainable params: 0\n","_________________________________________________________________\n","Shape of phi_I after reshape (None, 262144)\n","Shape of phi_I after division (None, 262144)\n","Shape of y_ssqrt (None, 262144)\n","Shape of z_l2 (None, 262144)\n","Model: \"model_12\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_14 (InputLayer)           [(None, 448, 448, 3) 0                                            \n","__________________________________________________________________________________________________\n","block1_conv1 (Conv2D)           (None, 448, 448, 64) 1792        input_14[0][0]                   \n","__________________________________________________________________________________________________\n","block1_conv2 (Conv2D)           (None, 448, 448, 64) 36928       block1_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block1_pool (MaxPooling2D)      (None, 224, 224, 64) 0           block1_conv2[0][0]               \n","__________________________________________________________________________________________________\n","block2_conv1 (Conv2D)           (None, 224, 224, 128 73856       block1_pool[0][0]                \n","__________________________________________________________________________________________________\n","block2_conv2 (Conv2D)           (None, 224, 224, 128 147584      block2_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block2_pool (MaxPooling2D)      (None, 112, 112, 128 0           block2_conv2[0][0]               \n","__________________________________________________________________________________________________\n","block3_conv1 (Conv2D)           (None, 112, 112, 256 295168      block2_pool[0][0]                \n","__________________________________________________________________________________________________\n","block3_conv2 (Conv2D)           (None, 112, 112, 256 590080      block3_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block3_conv3 (Conv2D)           (None, 112, 112, 256 590080      block3_conv2[0][0]               \n","__________________________________________________________________________________________________\n","block3_pool (MaxPooling2D)      (None, 56, 56, 256)  0           block3_conv3[0][0]               \n","__________________________________________________________________________________________________\n","block4_conv1 (Conv2D)           (None, 56, 56, 512)  1180160     block3_pool[0][0]                \n","__________________________________________________________________________________________________\n","block4_conv2 (Conv2D)           (None, 56, 56, 512)  2359808     block4_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block4_conv3 (Conv2D)           (None, 56, 56, 512)  2359808     block4_conv2[0][0]               \n","__________________________________________________________________________________________________\n","block4_pool (MaxPooling2D)      (None, 28, 28, 512)  0           block4_conv3[0][0]               \n","__________________________________________________________________________________________________\n","block5_conv1 (Conv2D)           (None, 28, 28, 512)  2359808     block4_pool[0][0]                \n","__________________________________________________________________________________________________\n","block5_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block5_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block5_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block5_conv2[0][0]               \n","__________________________________________________________________________________________________\n","tf.einsum_13 (TFOpLambda)       (None, 512, 512)     0           block5_conv3[0][0]               \n","                                                                 block4_pool[0][0]                \n","__________________________________________________________________________________________________\n","tf.reshape_13 (TFOpLambda)      (None, 262144)       0           tf.einsum_13[0][0]               \n","__________________________________________________________________________________________________\n","tf.math.truediv_13 (TFOpLambda) (None, 262144)       0           tf.reshape_13[0][0]              \n","__________________________________________________________________________________________________\n","tf.math.abs_13 (TFOpLambda)     (None, 262144)       0           tf.math.truediv_13[0][0]         \n","__________________________________________________________________________________________________\n","tf.__operators__.add_13 (TFOpLa (None, 262144)       0           tf.math.abs_13[0][0]             \n","__________________________________________________________________________________________________\n","tf.math.sign_13 (TFOpLambda)    (None, 262144)       0           tf.math.truediv_13[0][0]         \n","__________________________________________________________________________________________________\n","tf.math.sqrt_13 (TFOpLambda)    (None, 262144)       0           tf.__operators__.add_13[0][0]    \n","__________________________________________________________________________________________________\n","tf.math.multiply_13 (TFOpLambda (None, 262144)       0           tf.math.sign_13[0][0]            \n","                                                                 tf.math.sqrt_13[0][0]            \n","__________________________________________________________________________________________________\n","tf.math.l2_normalize_13 (TFOpLa (None, 262144)       0           tf.math.multiply_13[0][0]        \n","__________________________________________________________________________________________________\n","dense_12 (Dense)                (None, 24)           6291480     tf.math.l2_normalize_13[0][0]    \n","==================================================================================================\n","Total params: 21,006,168\n","Trainable params: 6,291,480\n","Non-trainable params: 14,714,688\n","__________________________________________________________________________________________________\n"],"name":"stdout"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-61a627fa68c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m# model.add(Dense(2, input_dim=1, activation='relu'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# model.add(Dense(1, activation='sigmoid'))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbcnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_layer_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi)\u001b[0m\n\u001b[1;32m    328\u001b[0m       \u001b[0mrankdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrankdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m       \u001b[0mexpand_nested\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpand_nested\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m       dpi=dpi)\n\u001b[0m\u001b[1;32m    331\u001b[0m   \u001b[0mto_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/vis_utils.py\u001b[0m in \u001b[0;36mmodel_to_dot\u001b[0;34m(model, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, subgraph)\u001b[0m\n\u001b[1;32m    234\u001b[0m       \u001b[0mnode_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_ib-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mnode_key\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minbound_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minbound_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m           \u001b[0minbound_layer_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minbound_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexpand_nested\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/node.py\u001b[0m in \u001b[0;36minbound_layers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     inbound_layers = nest.map_structure(lambda t: t._keras_history.layer,\n\u001b[0;32m--> 259\u001b[0;31m                                         self.call_args[0])\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minbound_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 867\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/node.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     inbound_layers = nest.map_structure(lambda t: t._keras_history.layer,\n\u001b[0m\u001b[1;32m    259\u001b[0m                                         self.call_args[0])\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minbound_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute '_keras_history'"]}]},{"cell_type":"markdown","metadata":{"id":"cRzk9GG0FBci"},"source":["Visualise the model"]},{"cell_type":"code","metadata":{"id":"y9Ct1IVLFBEh"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from keras.utils.vis_utils import plot_model\n","# model = Sequential()\n","\n","plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"riSf8UOOsCi8","executionInfo":{"status":"ok","timestamp":1625893807767,"user_tz":-330,"elapsed":13219,"user":{"displayName":"NEIL PARESH MEHTA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsKd7M0H1Ghtta64n6eNaGRg03HOsuCRSk0KNj1A=s64","userId":"02534006225616056494"}},"outputId":"4e040a19-543a-43ac-d153-00ea2806f588"},"source":["one_hot_labels_test = tf.keras.utils.to_categorical(y_test, num_classes=24)\n","test_loss,test_acc = model.evaluate(X_test,  one_hot_labels_test, verbose=2)\n","\n","print('\\nTest accuracy:', test_acc*100,\"%\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["24/24 - 13s - loss: 0.0150 - accuracy: 1.0000\n","\n","Test accuracy: 100.0 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DhEUzYe09Zyu"},"source":["Fine tuning"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NVRkTlYp9Y6S","executionInfo":{"status":"ok","timestamp":1625896936304,"user_tz":-330,"elapsed":744029,"user":{"displayName":"NEIL PARESH MEHTA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsKd7M0H1Ghtta64n6eNaGRg03HOsuCRSk0KNj1A=s64","userId":"02534006225616056494"}},"outputId":"42813996-af44-4e55-c8ab-0125605e4bc8"},"source":["# model_pretrained = tf.keras.applications.InceptionV3(weights=\"imagenet\",include_top=False,input_shape=(rows,cols,3))\n","# model_pretrained.summary()\n","\n","# inp = model_pretrained.input\n","# out = model_pretrained.layers[-2].output\n","\n","from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","import numpy as np\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","import tensorflow as tf\n","\n","# create the base pre-trained model\n","model_pretrained = VGG16(weights='imagenet', include_top=False,input_shape=(rows,cols,3))\n","model_pretrained.summary()\n","\n","# first: train only the top layers (which were randomly initialized)\n","# i.e. freeze all convolutional InceptionV3 layers except the last dense \n","# layer for 11 classes that was just added.\n","for layer in model_pretrained.layers[:15]:\n","    layer.trainable = False\n","for layer in model_pretrained.layers[15:]:\n","    layer.trainable = True\n","\n","x_5 = model_pretrained.get_layer(index=-2).output\n","x_5_3 = model_pretrained.get_layer(index=-5).output\n","added_layer = tf.einsum('ijkm,ijkn->imn',x_5,x_5_3)\n","x = tf.reshape(added_layer,[-1,512*512])\n","print('Shape of phi_I after reshape', x.get_shape())\n","\n","x = tf.divide(x,(784.0))  \n","print('Shape of phi_I after division', x.get_shape())\n","\n","a = tf.multiply(tf.sign(x),tf.sqrt(tf.abs(x)+1e-12))\n","print('Shape of y_ssqrt', a.get_shape())\n","\n","z = tf.nn.l2_normalize(a, dim=1)\n","print('Shape of z_l2', z.get_shape())\n","\n","\n","# x = GlobalAveragePooling2D()(x)\n","# let's add a fully-connected layer\n","# x = Dense(1024, activation='relu')(x)\n","# x = Dense(1024, activation='relu')(x)\n","\n","# and a logistic layer -- let's say we have 24 classes\n","predictions = Dense(numClasses, activation='softmax')(z)\n","\n","\n","\n","# this is the model we will train\n","model = Model(inputs=model_pretrained.input, outputs=predictions)\n","\n","# compile the model (should be done *after* setting layers to non-trainable)\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n","# y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n","# y_test = np.asarray(y_test).astype('float32').reshape((-1,1))\n","one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=numClasses)\n","model.summary()\n","model.fit(x=data,y=one_hot_labels,epochs=10,batch_size=32)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"vgg16\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_6 (InputLayer)         [(None, 448, 448, 3)]     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 448, 448, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 448, 448, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 224, 224, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 224, 224, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 224, 224, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 112, 112, 128)     0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 112, 112, 256)     295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 112, 112, 256)     590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 112, 112, 256)     590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 56, 56, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 56, 56, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 56, 56, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 56, 56, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 28, 28, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 14,714,688\n","Non-trainable params: 0\n","_________________________________________________________________\n","Shape of phi_I after reshape (None, 262144)\n","Shape of phi_I after division (None, 262144)\n","Shape of y_ssqrt (None, 262144)\n","Shape of z_l2 (None, 262144)\n","Model: \"model_5\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_6 (InputLayer)            [(None, 448, 448, 3) 0                                            \n","__________________________________________________________________________________________________\n","block1_conv1 (Conv2D)           (None, 448, 448, 64) 1792        input_6[0][0]                    \n","__________________________________________________________________________________________________\n","block1_conv2 (Conv2D)           (None, 448, 448, 64) 36928       block1_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block1_pool (MaxPooling2D)      (None, 224, 224, 64) 0           block1_conv2[0][0]               \n","__________________________________________________________________________________________________\n","block2_conv1 (Conv2D)           (None, 224, 224, 128 73856       block1_pool[0][0]                \n","__________________________________________________________________________________________________\n","block2_conv2 (Conv2D)           (None, 224, 224, 128 147584      block2_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block2_pool (MaxPooling2D)      (None, 112, 112, 128 0           block2_conv2[0][0]               \n","__________________________________________________________________________________________________\n","block3_conv1 (Conv2D)           (None, 112, 112, 256 295168      block2_pool[0][0]                \n","__________________________________________________________________________________________________\n","block3_conv2 (Conv2D)           (None, 112, 112, 256 590080      block3_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block3_conv3 (Conv2D)           (None, 112, 112, 256 590080      block3_conv2[0][0]               \n","__________________________________________________________________________________________________\n","block3_pool (MaxPooling2D)      (None, 56, 56, 256)  0           block3_conv3[0][0]               \n","__________________________________________________________________________________________________\n","block4_conv1 (Conv2D)           (None, 56, 56, 512)  1180160     block3_pool[0][0]                \n","__________________________________________________________________________________________________\n","block4_conv2 (Conv2D)           (None, 56, 56, 512)  2359808     block4_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block4_conv3 (Conv2D)           (None, 56, 56, 512)  2359808     block4_conv2[0][0]               \n","__________________________________________________________________________________________________\n","block4_pool (MaxPooling2D)      (None, 28, 28, 512)  0           block4_conv3[0][0]               \n","__________________________________________________________________________________________________\n","block5_conv1 (Conv2D)           (None, 28, 28, 512)  2359808     block4_pool[0][0]                \n","__________________________________________________________________________________________________\n","block5_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block5_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block5_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block5_conv2[0][0]               \n","__________________________________________________________________________________________________\n","tf.einsum_5 (TFOpLambda)        (None, 512, 512)     0           block5_conv3[0][0]               \n","                                                                 block4_pool[0][0]                \n","__________________________________________________________________________________________________\n","tf.reshape_5 (TFOpLambda)       (None, 262144)       0           tf.einsum_5[0][0]                \n","__________________________________________________________________________________________________\n","tf.math.truediv_5 (TFOpLambda)  (None, 262144)       0           tf.reshape_5[0][0]               \n","__________________________________________________________________________________________________\n","tf.math.abs_5 (TFOpLambda)      (None, 262144)       0           tf.math.truediv_5[0][0]          \n","__________________________________________________________________________________________________\n","tf.__operators__.add_5 (TFOpLam (None, 262144)       0           tf.math.abs_5[0][0]              \n","__________________________________________________________________________________________________\n","tf.math.sign_5 (TFOpLambda)     (None, 262144)       0           tf.math.truediv_5[0][0]          \n","__________________________________________________________________________________________________\n","tf.math.sqrt_5 (TFOpLambda)     (None, 262144)       0           tf.__operators__.add_5[0][0]     \n","__________________________________________________________________________________________________\n","tf.math.multiply_5 (TFOpLambda) (None, 262144)       0           tf.math.sign_5[0][0]             \n","                                                                 tf.math.sqrt_5[0][0]             \n","__________________________________________________________________________________________________\n","tf.math.l2_normalize_5 (TFOpLam (None, 262144)       0           tf.math.multiply_5[0][0]         \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 24)           6291480     tf.math.l2_normalize_5[0][0]     \n","==================================================================================================\n","Total params: 21,006,168\n","Trainable params: 13,370,904\n","Non-trainable params: 7,635,264\n","__________________________________________________________________________________________________\n","Epoch 1/10\n","120/120 [==============================] - 68s 562ms/step - loss: 1.6094 - accuracy: 0.6346\n","Epoch 2/10\n","120/120 [==============================] - 70s 587ms/step - loss: 0.2583 - accuracy: 0.9609\n","Epoch 3/10\n","120/120 [==============================] - 72s 602ms/step - loss: 0.0693 - accuracy: 0.9828\n","Epoch 4/10\n","120/120 [==============================] - 73s 610ms/step - loss: 0.0431 - accuracy: 0.9885\n","Epoch 5/10\n","120/120 [==============================] - 73s 609ms/step - loss: 0.0412 - accuracy: 0.9896\n","Epoch 6/10\n","120/120 [==============================] - 73s 609ms/step - loss: 0.0451 - accuracy: 0.9888\n","Epoch 7/10\n","120/120 [==============================] - 73s 610ms/step - loss: 0.0424 - accuracy: 0.9896\n","Epoch 8/10\n","120/120 [==============================] - 73s 611ms/step - loss: 0.0141 - accuracy: 0.9969\n","Epoch 9/10\n","120/120 [==============================] - 73s 612ms/step - loss: 0.0513 - accuracy: 0.9880\n","Epoch 10/10\n","120/120 [==============================] - 73s 612ms/step - loss: 0.0076 - accuracy: 0.9982\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f7b682899d0>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"yrHHlyOO9jZL"},"source":["one_hot_labels_test = tf.keras.utils.to_categorical(y_test, num_classes=24)\n","test_loss,test_acc = model.evaluate(X_test,  one_hot_labels_test, verbose=2)\n","\n","print('\\nTest accuracy:', test_acc*100,\"%\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eqx-wjFqwo82"},"source":["# x_5,x_5 (d+d)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LI30R0pet-jO","executionInfo":{"status":"ok","timestamp":1625894490597,"user_tz":-330,"elapsed":682855,"user":{"displayName":"NEIL PARESH MEHTA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsKd7M0H1Ghtta64n6eNaGRg03HOsuCRSk0KNj1A=s64","userId":"02534006225616056494"}},"outputId":"cd31d551-7afd-489e-e3a5-63aaa1289227"},"source":["# model_pretrained = tf.keras.applications.InceptionV3(weights=\"imagenet\",include_top=False,input_shape=(rows,cols,3))\n","# model_pretrained.summary()\n","\n","# inp = model_pretrained.input\n","# out = model_pretrained.layers[-2].output\n","\n","from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","import numpy as np\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","import tensorflow as tf\n","\n","# create the base pre-trained model\n","model_pretrained = VGG16(weights='imagenet', include_top=False,input_shape=(rows,cols,3))\n","model_pretrained.summary()\n","\n","# first: train only the top layers (which were randomly initialized)\n","# i.e. freeze all convolutional InceptionV3 layers except the last dense \n","# layer for 11 classes that was just added.\n","for layer in model_pretrained.layers:\n","    layer.trainable = False\n","\n","x_5 = model_pretrained.get_layer(index=-2).output\n","x_5_3 = model_pretrained.get_layer(index=-5).output\n","added_layer = tf.einsum('ijkm,ijkn->imn',x_5,x_5)\n","x = tf.reshape(added_layer,[-1,512*512])\n","print('Shape of phi_I after reshape', x.get_shape())\n","\n","x = tf.divide(x,(784.0))  \n","print('Shape of phi_I after division', x.get_shape())\n","\n","a = tf.multiply(tf.sign(x),tf.sqrt(tf.abs(x)+1e-12))\n","print('Shape of y_ssqrt', a.get_shape())\n","\n","z = tf.nn.l2_normalize(a, dim=1)\n","print('Shape of z_l2', z.get_shape())\n","\n","\n","# x = GlobalAveragePooling2D()(x)\n","# let's add a fully-connected layer\n","# x = Dense(1024, activation='relu')(x)\n","# x = Dense(1024, activation='relu')(x)\n","\n","# and a logistic layer -- let's say we have 24 classes\n","predictions = Dense(numClasses, activation='softmax')(z)\n","\n","\n","\n","# this is the model we will train\n","model = Model(inputs=model_pretrained.input, outputs=predictions)\n","\n","# compile the model (should be done *after* setting layers to non-trainable)\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n","# y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n","# y_test = np.asarray(y_test).astype('float32').reshape((-1,1))\n","one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=numClasses)\n","model.summary()\n","model.fit(x=data,y=one_hot_labels,epochs=10,batch_size=32)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"vgg16\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_4 (InputLayer)         [(None, 448, 448, 3)]     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 448, 448, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 448, 448, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 224, 224, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 224, 224, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 224, 224, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 112, 112, 128)     0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 112, 112, 256)     295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 112, 112, 256)     590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 112, 112, 256)     590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 56, 56, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 56, 56, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 56, 56, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 56, 56, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 28, 28, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 14,714,688\n","Non-trainable params: 0\n","_________________________________________________________________\n","Shape of phi_I after reshape (None, 262144)\n","Shape of phi_I after division (None, 262144)\n","Shape of y_ssqrt (None, 262144)\n","Shape of z_l2 (None, 262144)\n","Model: \"model_3\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_4 (InputLayer)            [(None, 448, 448, 3) 0                                            \n","__________________________________________________________________________________________________\n","block1_conv1 (Conv2D)           (None, 448, 448, 64) 1792        input_4[0][0]                    \n","__________________________________________________________________________________________________\n","block1_conv2 (Conv2D)           (None, 448, 448, 64) 36928       block1_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block1_pool (MaxPooling2D)      (None, 224, 224, 64) 0           block1_conv2[0][0]               \n","__________________________________________________________________________________________________\n","block2_conv1 (Conv2D)           (None, 224, 224, 128 73856       block1_pool[0][0]                \n","__________________________________________________________________________________________________\n","block2_conv2 (Conv2D)           (None, 224, 224, 128 147584      block2_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block2_pool (MaxPooling2D)      (None, 112, 112, 128 0           block2_conv2[0][0]               \n","__________________________________________________________________________________________________\n","block3_conv1 (Conv2D)           (None, 112, 112, 256 295168      block2_pool[0][0]                \n","__________________________________________________________________________________________________\n","block3_conv2 (Conv2D)           (None, 112, 112, 256 590080      block3_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block3_conv3 (Conv2D)           (None, 112, 112, 256 590080      block3_conv2[0][0]               \n","__________________________________________________________________________________________________\n","block3_pool (MaxPooling2D)      (None, 56, 56, 256)  0           block3_conv3[0][0]               \n","__________________________________________________________________________________________________\n","block4_conv1 (Conv2D)           (None, 56, 56, 512)  1180160     block3_pool[0][0]                \n","__________________________________________________________________________________________________\n","block4_conv2 (Conv2D)           (None, 56, 56, 512)  2359808     block4_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block4_conv3 (Conv2D)           (None, 56, 56, 512)  2359808     block4_conv2[0][0]               \n","__________________________________________________________________________________________________\n","block4_pool (MaxPooling2D)      (None, 28, 28, 512)  0           block4_conv3[0][0]               \n","__________________________________________________________________________________________________\n","block5_conv1 (Conv2D)           (None, 28, 28, 512)  2359808     block4_pool[0][0]                \n","__________________________________________________________________________________________________\n","block5_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block5_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block5_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block5_conv2[0][0]               \n","__________________________________________________________________________________________________\n","tf.einsum_3 (TFOpLambda)        (None, 512, 512)     0           block5_conv3[0][0]               \n","                                                                 block5_conv3[0][0]               \n","__________________________________________________________________________________________________\n","tf.reshape_3 (TFOpLambda)       (None, 262144)       0           tf.einsum_3[0][0]                \n","__________________________________________________________________________________________________\n","tf.math.truediv_3 (TFOpLambda)  (None, 262144)       0           tf.reshape_3[0][0]               \n","__________________________________________________________________________________________________\n","tf.math.abs_3 (TFOpLambda)      (None, 262144)       0           tf.math.truediv_3[0][0]          \n","__________________________________________________________________________________________________\n","tf.__operators__.add_3 (TFOpLam (None, 262144)       0           tf.math.abs_3[0][0]              \n","__________________________________________________________________________________________________\n","tf.math.sign_3 (TFOpLambda)     (None, 262144)       0           tf.math.truediv_3[0][0]          \n","__________________________________________________________________________________________________\n","tf.math.sqrt_3 (TFOpLambda)     (None, 262144)       0           tf.__operators__.add_3[0][0]     \n","__________________________________________________________________________________________________\n","tf.math.multiply_3 (TFOpLambda) (None, 262144)       0           tf.math.sign_3[0][0]             \n","                                                                 tf.math.sqrt_3[0][0]             \n","__________________________________________________________________________________________________\n","tf.math.l2_normalize_3 (TFOpLam (None, 262144)       0           tf.math.multiply_3[0][0]         \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 24)           6291480     tf.math.l2_normalize_3[0][0]     \n","==================================================================================================\n","Total params: 21,006,168\n","Trainable params: 6,291,480\n","Non-trainable params: 14,714,688\n","__________________________________________________________________________________________________\n","Epoch 1/10\n","120/120 [==============================] - 67s 551ms/step - loss: 1.9373 - accuracy: 0.7919\n","Epoch 2/10\n","120/120 [==============================] - 66s 550ms/step - loss: 0.8304 - accuracy: 0.9669\n","Epoch 3/10\n","120/120 [==============================] - 66s 549ms/step - loss: 0.4493 - accuracy: 0.9844\n","Epoch 4/10\n","120/120 [==============================] - 66s 548ms/step - loss: 0.2777 - accuracy: 0.9901\n","Epoch 5/10\n","120/120 [==============================] - 66s 550ms/step - loss: 0.1851 - accuracy: 0.9932\n","Epoch 6/10\n","120/120 [==============================] - 66s 548ms/step - loss: 0.1302 - accuracy: 0.9961\n","Epoch 7/10\n","120/120 [==============================] - 66s 548ms/step - loss: 0.0955 - accuracy: 0.9971\n","Epoch 8/10\n","120/120 [==============================] - 66s 547ms/step - loss: 0.0724 - accuracy: 0.9984\n","Epoch 9/10\n","120/120 [==============================] - 66s 549ms/step - loss: 0.0565 - accuracy: 0.9990\n","Epoch 10/10\n","120/120 [==============================] - 66s 550ms/step - loss: 0.0446 - accuracy: 0.9992\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f7c0823a0d0>"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YUkYJiSiwwtN","executionInfo":{"status":"ok","timestamp":1625894511238,"user_tz":-330,"elapsed":20668,"user":{"displayName":"NEIL PARESH MEHTA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsKd7M0H1Ghtta64n6eNaGRg03HOsuCRSk0KNj1A=s64","userId":"02534006225616056494"}},"outputId":"21f084f0-d26c-49cc-a708-5baa349507fd"},"source":["one_hot_labels_test = tf.keras.utils.to_categorical(y_test, num_classes=24)\n","test_loss,test_acc = model.evaluate(X_test,  one_hot_labels_test, verbose=2)\n","\n","print('\\nTest accuracy:', test_acc*100,\"%\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["24/24 - 13s - loss: 0.0354 - accuracy: 1.0000\n","\n","Test accuracy: 100.0 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iqCtKgRN921i"},"source":["Fine tuning"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FmkIixinxyKL","executionInfo":{"status":"ok","timestamp":1625897731119,"user_tz":-330,"elapsed":791337,"user":{"displayName":"NEIL PARESH MEHTA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsKd7M0H1Ghtta64n6eNaGRg03HOsuCRSk0KNj1A=s64","userId":"02534006225616056494"}},"outputId":"dbb34bad-1fa0-45be-da5f-5b70d292f13c"},"source":["# model_pretrained = tf.keras.applications.InceptionV3(weights=\"imagenet\",include_top=False,input_shape=(rows,cols,3))\n","# model_pretrained.summary()\n","\n","# inp = model_pretrained.input\n","# out = model_pretrained.layers[-2].output\n","\n","from tensorflow.keras.applications.vgg16 import VGG16\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","import numpy as np\n","from tensorflow.keras.preprocessing import image\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","import tensorflow as tf\n","\n","# create the base pre-trained model\n","model_pretrained = VGG16(weights='imagenet', include_top=False,input_shape=(rows,cols,3))\n","model_pretrained.summary()\n","\n","# first: train only the top layers (which were randomly initialized)\n","# i.e. freeze all convolutional InceptionV3 layers except the last dense \n","# layer for 11 classes that was just added.\n","for layer in model_pretrained.layers[:15]:\n","    layer.trainable = False\n","for layer in model_pretrained.layers[15:]:\n","    layer.trainable = True\n","\n","x_5 = model_pretrained.get_layer(index=-2).output\n","x_5_3 = model_pretrained.get_layer(index=-5).output\n","added_layer = tf.einsum('ijkm,ijkn->imn',x_5,x_5)\n","x = tf.reshape(added_layer,[-1,512*512])\n","print('Shape of phi_I after reshape', x.get_shape())\n","\n","x = tf.divide(x,(784.0))  \n","print('Shape of phi_I after division', x.get_shape())\n","\n","a = tf.multiply(tf.sign(x),tf.sqrt(tf.abs(x)+1e-12))\n","print('Shape of y_ssqrt', a.get_shape())\n","\n","z = tf.nn.l2_normalize(a, dim=1)\n","print('Shape of z_l2', z.get_shape())\n","\n","\n","# x = GlobalAveragePooling2D()(x)\n","# let's add a fully-connected layer\n","# x = Dense(1024, activation='relu')(x)\n","# x = Dense(1024, activation='relu')(x)\n","\n","# and a logistic layer -- let's say we have 24 classes\n","predictions = Dense(numClasses, activation='softmax')(z)\n","\n","\n","\n","# this is the model we will train\n","model = Model(inputs=model_pretrained.input, outputs=predictions)\n","\n","# compile the model (should be done *after* setting layers to non-trainable)\n","model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n","# y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n","# y_test = np.asarray(y_test).astype('float32').reshape((-1,1))\n","one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=numClasses)\n","model.summary()\n","model.fit(x=data,y=one_hot_labels,epochs=10,batch_size=32)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Model: \"vgg16\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_7 (InputLayer)         [(None, 448, 448, 3)]     0         \n","_________________________________________________________________\n","block1_conv1 (Conv2D)        (None, 448, 448, 64)      1792      \n","_________________________________________________________________\n","block1_conv2 (Conv2D)        (None, 448, 448, 64)      36928     \n","_________________________________________________________________\n","block1_pool (MaxPooling2D)   (None, 224, 224, 64)      0         \n","_________________________________________________________________\n","block2_conv1 (Conv2D)        (None, 224, 224, 128)     73856     \n","_________________________________________________________________\n","block2_conv2 (Conv2D)        (None, 224, 224, 128)     147584    \n","_________________________________________________________________\n","block2_pool (MaxPooling2D)   (None, 112, 112, 128)     0         \n","_________________________________________________________________\n","block3_conv1 (Conv2D)        (None, 112, 112, 256)     295168    \n","_________________________________________________________________\n","block3_conv2 (Conv2D)        (None, 112, 112, 256)     590080    \n","_________________________________________________________________\n","block3_conv3 (Conv2D)        (None, 112, 112, 256)     590080    \n","_________________________________________________________________\n","block3_pool (MaxPooling2D)   (None, 56, 56, 256)       0         \n","_________________________________________________________________\n","block4_conv1 (Conv2D)        (None, 56, 56, 512)       1180160   \n","_________________________________________________________________\n","block4_conv2 (Conv2D)        (None, 56, 56, 512)       2359808   \n","_________________________________________________________________\n","block4_conv3 (Conv2D)        (None, 56, 56, 512)       2359808   \n","_________________________________________________________________\n","block4_pool (MaxPooling2D)   (None, 28, 28, 512)       0         \n","_________________________________________________________________\n","block5_conv1 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block5_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block5_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n","_________________________________________________________________\n","block5_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 14,714,688\n","Non-trainable params: 0\n","_________________________________________________________________\n","Shape of phi_I after reshape (None, 262144)\n","Shape of phi_I after division (None, 262144)\n","Shape of y_ssqrt (None, 262144)\n","Shape of z_l2 (None, 262144)\n","Model: \"model_6\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_7 (InputLayer)            [(None, 448, 448, 3) 0                                            \n","__________________________________________________________________________________________________\n","block1_conv1 (Conv2D)           (None, 448, 448, 64) 1792        input_7[0][0]                    \n","__________________________________________________________________________________________________\n","block1_conv2 (Conv2D)           (None, 448, 448, 64) 36928       block1_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block1_pool (MaxPooling2D)      (None, 224, 224, 64) 0           block1_conv2[0][0]               \n","__________________________________________________________________________________________________\n","block2_conv1 (Conv2D)           (None, 224, 224, 128 73856       block1_pool[0][0]                \n","__________________________________________________________________________________________________\n","block2_conv2 (Conv2D)           (None, 224, 224, 128 147584      block2_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block2_pool (MaxPooling2D)      (None, 112, 112, 128 0           block2_conv2[0][0]               \n","__________________________________________________________________________________________________\n","block3_conv1 (Conv2D)           (None, 112, 112, 256 295168      block2_pool[0][0]                \n","__________________________________________________________________________________________________\n","block3_conv2 (Conv2D)           (None, 112, 112, 256 590080      block3_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block3_conv3 (Conv2D)           (None, 112, 112, 256 590080      block3_conv2[0][0]               \n","__________________________________________________________________________________________________\n","block3_pool (MaxPooling2D)      (None, 56, 56, 256)  0           block3_conv3[0][0]               \n","__________________________________________________________________________________________________\n","block4_conv1 (Conv2D)           (None, 56, 56, 512)  1180160     block3_pool[0][0]                \n","__________________________________________________________________________________________________\n","block4_conv2 (Conv2D)           (None, 56, 56, 512)  2359808     block4_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block4_conv3 (Conv2D)           (None, 56, 56, 512)  2359808     block4_conv2[0][0]               \n","__________________________________________________________________________________________________\n","block4_pool (MaxPooling2D)      (None, 28, 28, 512)  0           block4_conv3[0][0]               \n","__________________________________________________________________________________________________\n","block5_conv1 (Conv2D)           (None, 28, 28, 512)  2359808     block4_pool[0][0]                \n","__________________________________________________________________________________________________\n","block5_conv2 (Conv2D)           (None, 28, 28, 512)  2359808     block5_conv1[0][0]               \n","__________________________________________________________________________________________________\n","block5_conv3 (Conv2D)           (None, 28, 28, 512)  2359808     block5_conv2[0][0]               \n","__________________________________________________________________________________________________\n","tf.einsum_6 (TFOpLambda)        (None, 512, 512)     0           block5_conv3[0][0]               \n","                                                                 block5_conv3[0][0]               \n","__________________________________________________________________________________________________\n","tf.reshape_6 (TFOpLambda)       (None, 262144)       0           tf.einsum_6[0][0]                \n","__________________________________________________________________________________________________\n","tf.math.truediv_6 (TFOpLambda)  (None, 262144)       0           tf.reshape_6[0][0]               \n","__________________________________________________________________________________________________\n","tf.math.abs_6 (TFOpLambda)      (None, 262144)       0           tf.math.truediv_6[0][0]          \n","__________________________________________________________________________________________________\n","tf.__operators__.add_6 (TFOpLam (None, 262144)       0           tf.math.abs_6[0][0]              \n","__________________________________________________________________________________________________\n","tf.math.sign_6 (TFOpLambda)     (None, 262144)       0           tf.math.truediv_6[0][0]          \n","__________________________________________________________________________________________________\n","tf.math.sqrt_6 (TFOpLambda)     (None, 262144)       0           tf.__operators__.add_6[0][0]     \n","__________________________________________________________________________________________________\n","tf.math.multiply_6 (TFOpLambda) (None, 262144)       0           tf.math.sign_6[0][0]             \n","                                                                 tf.math.sqrt_6[0][0]             \n","__________________________________________________________________________________________________\n","tf.math.l2_normalize_6 (TFOpLam (None, 262144)       0           tf.math.multiply_6[0][0]         \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 24)           6291480     tf.math.l2_normalize_6[0][0]     \n","==================================================================================================\n","Total params: 21,006,168\n","Trainable params: 13,370,904\n","Non-trainable params: 7,635,264\n","__________________________________________________________________________________________________\n","Epoch 1/10\n","120/120 [==============================] - 117s 579ms/step - loss: 1.9452 - accuracy: 0.4151\n","Epoch 2/10\n","120/120 [==============================] - 72s 603ms/step - loss: 0.3945 - accuracy: 0.9133\n","Epoch 3/10\n","120/120 [==============================] - 74s 614ms/step - loss: 0.1104 - accuracy: 0.9708\n","Epoch 4/10\n","120/120 [==============================] - 73s 610ms/step - loss: 0.0589 - accuracy: 0.9831\n","Epoch 5/10\n","120/120 [==============================] - 73s 610ms/step - loss: 0.0623 - accuracy: 0.9852\n","Epoch 6/10\n","120/120 [==============================] - 73s 612ms/step - loss: 0.0524 - accuracy: 0.9883\n","Epoch 7/10\n","120/120 [==============================] - 74s 613ms/step - loss: 0.0378 - accuracy: 0.9901\n","Epoch 8/10\n","120/120 [==============================] - 73s 610ms/step - loss: 0.0289 - accuracy: 0.9940\n","Epoch 9/10\n","120/120 [==============================] - 73s 612ms/step - loss: 0.0363 - accuracy: 0.9896\n","Epoch 10/10\n","120/120 [==============================] - 74s 613ms/step - loss: 0.0127 - accuracy: 0.9964\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f7b180a1690>"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jhB83a8m9_gT","executionInfo":{"status":"ok","timestamp":1625897743687,"user_tz":-330,"elapsed":12657,"user":{"displayName":"NEIL PARESH MEHTA","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhsKd7M0H1Ghtta64n6eNaGRg03HOsuCRSk0KNj1A=s64","userId":"02534006225616056494"}},"outputId":"c7693ac7-61e9-440b-9360-39ee0e9c8fcf"},"source":["one_hot_labels_test = tf.keras.utils.to_categorical(y_test, num_classes=24)\n","test_loss,test_acc = model.evaluate(X_test,  one_hot_labels_test, verbose=2)\n","\n","print('\\nTest accuracy:', test_acc*100,\"%\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["24/24 - 13s - loss: 1.3144e-04 - accuracy: 1.0000\n","\n","Test accuracy: 100.0 %\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wJI0vbIo-EKE"},"source":[""],"execution_count":null,"outputs":[]}]}